{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Iterable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "import sys\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads # dimension of each head\n",
    "\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # output layer\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None):\n",
    "        b = query.shape[0] # batch size\n",
    "        \n",
    "        q = self.W_q(query).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2) # (b, num_heads, seq_len, head_dim)\n",
    "        k = self.W_k(key).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.W_v(value).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        dot_product_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            dot_product_score = dot_product_score.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_scores = F.softmax(dot_product_score, dim=-1)\n",
    "        out = torch.matmul(attention_scores, v)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(b, -1, self.embed_dim)\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.pe = self.compute_positional_encoding()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        input_dim = x.shape[2]\n",
    "\n",
    "        pe = self.pe[:, :seq_len, :]\n",
    "        x = x + pe.to(x.device)\n",
    "        return x\n",
    "    \n",
    "    def compute_positional_encoding(self):\n",
    "        position = torch.arange(0, self.max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, self.embed_dim, 2).float() * (-math.log(10000.0) / self.embed_dim))\n",
    "        pe = torch.zeros(self.max_len, self.embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, dropout=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerEncoderCell(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout=0.1):\n",
    "        super(TransformerEncoderCell, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward_network = FeedForwardNetwork(embed_dim, hidden_dim, dropout)\n",
    "\n",
    "        self.norm_attention = nn.LayerNorm(embed_dim)\n",
    "        self.norm_ffn = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # multi-head attention\n",
    "        attention_output = self.multi_head_attention(x, x, x, mask)\n",
    "        \n",
    "        x = x + self.dropout(attention_output)\n",
    "        x = self.norm_attention(x)\n",
    "\n",
    "        # feed forward network\n",
    "        ffn_output = self.feed_forward_network(x)\n",
    "        y = x + self.dropout(ffn_output)\n",
    "        y = self.norm_ffn(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, hidden_dim, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.encoder_cells = nn.ModuleList([TransformerEncoderCell(embed_dim, num_heads, hidden_dim, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for encoder_cell in self.encoder_cells:\n",
    "            x = encoder_cell(x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_layers, embed_dim, num_heads, hidden_dim, num_classes, dropout=0.1, pad_token:int=0):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_token)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "        self.encoder = TransformerEncoder(num_layers, embed_dim, num_heads, hidden_dim, dropout)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        embedded = self.embedding(text) * math.sqrt(self.embed_dim)\n",
    "        position_encoded = self.positional_encoding(embedded)\n",
    "        transformer_output = self.encoder(position_encoded, mask)\n",
    "        \n",
    "        # Average Pool\n",
    "        x = torch.mean(transformer_output, dim=1)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data = load_dataset('csv', data_files=data_dir, split='train')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, text = self.data[idx]['output'], self.data[idx]['input']\n",
    "        return label, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "dataset_name = \"../../data/lm_finetune_data/ctx-only_train.csv\"\n",
    "train_iter = TextClassificationDataset(dataset_name)\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, tokenizer):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter, tokenizer), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "PAD_TOKEN = vocab(tokenizer('<pad>'))\n",
    "assert len(PAD_TOKEN) == 1\n",
    "PAD_TOKEN = PAD_TOKEN[0]\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, text_len_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        text_len_list.append(processed_text.size(0))\n",
    "\n",
    "    max_len = max(text_len_list)\n",
    "    padded_text_list = [F.pad(text, pad=(0, max_len - len(text)), value=PAD_TOKEN) for text in text_list]\n",
    "\n",
    "    batched_label, batched_text = torch.tensor(label_list), torch.stack(padded_text_list, dim=0)\n",
    "    return batched_label, batched_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decide Hyperparameters and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_func, device, grad_norm_clip, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    global logits_checker\n",
    "    total_predictions = None\n",
    "    total_labels = None\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        label = label + 1\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(text, mask=None)\n",
    "        loss = loss_func(logits, label)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "        if idx == 0:\n",
    "            total_predictions = logits.argmax(1)\n",
    "            total_labels = label\n",
    "        else:\n",
    "            total_predictions = torch.cat((total_predictions, logits.argmax(1)), 0)\n",
    "            total_labels = torch.cat((total_labels, label), 0)\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | positive precision {:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_predictions = None\n",
    "    total_labels = None\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label = label + 1\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "            logits = model(text, mask=None)\n",
    "            loss = loss_func(logits, label)\n",
    "            if idx == 0:\n",
    "                total_predictions = logits.argmax(1)\n",
    "                total_labels = label\n",
    "            else:\n",
    "                total_predictions = torch.cat((total_predictions, logits.argmax(1)), 0)\n",
    "                total_labels = torch.cat((total_labels, label), 0)\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    \n",
    "    print(\"Num of positive labels: \", total_labels.sum())\n",
    "    print(\"Num of labels: \", total_labels.size(0))\n",
    "\n",
    "    # calculate the confusion matrix\n",
    "    cm = confusion_matrix(total_labels.cpu().numpy(), total_predictions.cpu().numpy())\n",
    "    tn,fp,fn,tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)    \n",
    "\n",
    "    print(f\"Positive Precision: {precision}\")\n",
    "    print(f\"Positive Recall: {recall}\")\n",
    "    f1 = f1_score(total_labels.cpu().numpy(), total_predictions.cpu().numpy(), average=None)\n",
    "    print(f\"Positive F1: {f1}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(total_labels.cpu().numpy(), total_predictions.cpu().numpy())\n",
    "    print(f\"Area Under Curve: {auc(fpr, tpr)}\")\n",
    "    print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")\n",
    "    return total_acc / total_count, f1[1]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 50\n",
    "lr = 0.0005\n",
    "batch_size = 256\n",
    "\n",
    "dataset_name = \"../../data/lm_finetune_data/context-only_train.csv\"\n",
    "validation_dataset_name = \"../../data/lm_finetune_data/context-only_dev.csv\"\n",
    "train_iter = TextClassificationDataset(dataset_name)\n",
    "validation_iter = TextClassificationDataset(validation_dataset_name)\n",
    "num_classes = len(set([label for label, _ in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "gradient_norm_clip = 1.0\n",
    "emb_size = 64\n",
    "\n",
    "def model_training(epochs, lr, batch_size, num_layers, num_heads, emb_size):\n",
    "    sys.stdout = open(f'../../outputs/MHA/output_e_{epochs}_lr_{lr}_b_{batch_size}_heads_{num_heads}_layers_{num_layers}_emb_{emb_size}.txt', 'w', buffering=1)\n",
    "    print(f\"Running with epochs: {epochs}, lr: {lr}, batch_size: {batch_size}, num_heads: {num_heads}, num_layers: {num_layers}, emb_size: {emb_size}\")\n",
    "    model = TransformerClassifier(vocab_size=vocab_size, \n",
    "                                            num_layers=num_layers, \n",
    "                                            embed_dim=emb_size, \n",
    "                                            num_heads=num_heads, \n",
    "                                            hidden_dim=emb_size, \n",
    "                                            num_classes=num_classes)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-8)\n",
    "    total_accu = None\n",
    "\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    validation_dataloader = DataLoader(validation_iter, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, train_dataloader, loss_fn, device, gradient_norm_clip, optimizer, epoch)\n",
    "        accu_val, f1 = evaluate(model, validation_dataloader, loss_fn, device)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            model.save_state_dict(f\"results/MHA/model_e_{epochs}_lr_{lr}_b_{batch_size}_heads_{num_heads}_layers_{num_layers}_emb_{emb_size}.pt\")\n",
    "        if total_accu is not None and total_accu > accu_val:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            total_accu = accu_val\n",
    "        print('-' * 59)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} '.format(epoch, (time.time() - epoch_start_time), accu_val))\n",
    "        print('-' * 59)\n",
    "    print(f\"Best validation F1: {best_f1}\")\n",
    "\n",
    "\n",
    "for num_layers in range(4,13):\n",
    "    for num_heads in [2,4]:\n",
    "        for emb_idx, emb_size in enumerate([64, 128, 256]):\n",
    "            model_training(epochs, lr, batch_size, num_layers, num_heads, emb_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn2aug2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
