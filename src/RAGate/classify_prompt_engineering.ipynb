{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"YOUR_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_dir = \"../data/lm_finetune_data/ctx-resp_train.csv\"\n",
    "data = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    set_seed\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Transformer parameters. you can vary the model choice to conduct prompt generation with different models\n",
    "\n",
    "model_name = \"wxjiao/alpaca-7b\"\n",
    "\n",
    "# Bitsandbytes parameters\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit model \n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# The quantization type for 4-bit model (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# The compute dtype for 4-bit model\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
    "def create_bnb_config(load_in_4bit, bnb_4_bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype):\n",
    "    \"\"\"\n",
    "        Configure model quantization using bitsandbytes to speed up training and inference\n",
    "        :param load_in_4bit: Load the model in 4-bit precision mode\n",
    "        :param bnb_4_bit_use_double_quant: nested quantization for 4-bit model\n",
    "        :param bnb_4bit_quant_type: The quantization type for 4-bit model\n",
    "        :param bnb_4bit_compute_dtype: The compute dtype for 4-bit model\n",
    "    \"\"\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype\n",
    "    )\n",
    "    return bnb_config\n",
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    \"\"\"\n",
    "        Load the model and tokenizer\n",
    "        :param model_name: Huggingface model name\n",
    "        :param bnb_config: Bitsandbytes configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of GPU device and set maximum memory\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{22960}MB'\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 device_map = \"auto\",\n",
    "                                                 max_memory= {i: max_memory for i in range(n_gpus)})\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle as p\n",
    "\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "        Create a formatted prompt template for a prompt in the instruction dataset\n",
    "    \"\"\"\n",
    "    # Initialize static strings for the prompt template\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Please respond with 'True' or 'False' only that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # Combine a prompt with the static strings\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    example = \"USER: I'm planning a trip, can you help me look for a flight? SYSTEM: Which day are you planning to return and from which city? USER: I want to go from NYC the day after tomorrow and return on the 13th of this month. SYSTEM: Where would you like to go? USER: I want to go to Vancouver, BC. Can you look for a Premium Economy class ticket. SYSTEM: I found 1 flight for you. It is a Delta Airlines flight that takes off at 6 am and returns at 2:50 am. The price is $505. USER: What is the departure airport, and how many stops does the flight have?\"\n",
    "    example_2 = \"USER: Get me bus tickets to an Cher event on March 6th SYSTEM: How many to buy? USER: only one please\"\n",
    "    input_text = f\"{INPUT_KEY}\\n{sample['input']}\" if sample['input'] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "\n",
    "    # Create a list of prompt template elements\n",
    "    # parts = [part for part in [blurb, instruction, \"Example 1: \", example, \"### Response: True\", \"Example 2: \", example_2, \"### Response: False\", input_text, response, end] if part]\n",
    "    parts = [part for part in [blurb, instruction, input_text, response, end] if part]\n",
    "    # Combine the prompt template elements into a single string\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "predictions = []\n",
    "for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    prompt = create_prompt_formats(row)[\"text\"]\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    output = model.generate(input_ids, max_length=(input_ids.shape[1] + 20)) \n",
    "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    output_text = output_text[len(prompt):].lower()\n",
    "    if 'false' in output_text:\n",
    "        predictions.append(\"False\")\n",
    "    elif 'true' in output_text:\n",
    "        predictions.append(\"True\")\n",
    "    else:\n",
    "        predictions.append(\"error\")\n",
    "\n",
    "p.dump(predictions, open(\"../../outputs/prompt_pred/zero_alpaca-7b_prediction.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import pickle as p\n",
    "\n",
    "labels = []\n",
    "for i, row in data.iterrows():\n",
    "    labels.append(row['output'])\n",
    "\n",
    "converted_outputs = []\n",
    "for output in predictions:\n",
    "    converted_outputs.extend([1 if output == 'True' else 0])\n",
    "# Calculate the precision, recall, F1, and support\n",
    "print(precision_score(labels, converted_outputs, average=None))\n",
    "\n",
    "conf_matrix = confusion_matrix(labels, converted_outputs)\n",
    "tn,fp,fn,tp = conf_matrix.ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "\n",
    "\n",
    "tn,fp,fn,tp = conf_matrix.ravel()\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)    \n",
    "\n",
    "print(f\"Positive Precision: {precision}\")\n",
    "print(f\"Positive Recall: {recall}\")\n",
    "f1 = f1_score(labels, converted_outputs, average=None)\n",
    "print(f\"Positive F1: {f1}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels, converted_outputs)\n",
    "print(f\"Area Under Curve: {auc(fpr, tpr)}\")\n",
    "print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn2aug2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
